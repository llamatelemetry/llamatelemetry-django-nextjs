export const recipeTags = [
  "CUDA",
  "GGUF",
  "llama.cpp",
  "OTLP",
  "Grafana",
  "Traceloop",
  "Graphistry",
  "Kaggle",
  "Local",
  "Collector",
];

export const recipes = [
  {
    id: "quickstart-local",
    title: "Local GPU quickstart",
    summary: "Install, run llama-server, and capture traces locally in under 10 minutes.",
    time: "10 min",
    tags: ["CUDA", "GGUF", "llama.cpp", "Local"],
    steps: [
      "Install llamatelemetry + optional CUDA extras",
      "Run llama-server with a GGUF model",
      "Enable OTLP exporter to localhost",
      "Open Grafana/Traceloop to confirm trace flow",
    ],
    href: "/get-started/quickstart",
  },
  {
    id: "otlp-grafana-cloud",
    title: "OTLP → Grafana Cloud",
    summary: "Export spans and metrics directly to Grafana Cloud OTLP endpoint.",
    time: "15 min",
    tags: ["OTLP", "Grafana"],
    steps: [
      "Create a Grafana Cloud stack",
      "Copy OTLP endpoint + auth token",
      "Set OTLP env vars in llamatelemetry",
      "Confirm traces in Grafana Explore",
    ],
    href: "/guides/telemetry-observability",
  },
  {
    id: "otlp-local-collector",
    title: "OTLP → local collector",
    summary: "Route telemetry into a local OpenTelemetry Collector pipeline.",
    time: "12 min",
    tags: ["OTLP", "Collector", "Local"],
    steps: [
      "Start a local OTEL collector",
      "Configure OTLP HTTP/gRPC endpoint",
      "Set llamatelemetry exporter endpoint",
      "Forward to your backend of choice",
    ],
    href: "/guides/telemetry-observability",
  },
  {
    id: "graphistry-inference-graph",
    title: "Graphistry inference graph",
    summary: "Visualize token flow and inference spans as a graph.",
    time: "20 min",
    tags: ["Graphistry", "OTLP"],
    steps: [
      "Enable Graphistry integration",
      "Capture inference graph events",
      "Render graph with filtering",
      "Export link to share",
    ],
    href: "/guides/graphistry-rapids",
  },
  {
    id: "kaggle-notebook-pipeline",
    title: "Kaggle notebook pipeline",
    summary: "Run the full notebook track with dual T4 GPUs and telemetry.",
    time: "30 min",
    tags: ["Kaggle", "CUDA", "GGUF"],
    steps: [
      "Open Kaggle quickstart notebook",
      "Install with GPU extras",
      "Run inference + telemetry cells",
      "Export traces for analysis",
    ],
    href: "/get-started/kaggle-quickstart",
  },
  {
    id: "server-ops",
    title: "Server management + observability",
    summary: "Operate llama-server with lifecycle controls and telemetry hooks.",
    time: "18 min",
    tags: ["llama.cpp", "OTLP"],
    steps: [
      "Configure ServerManager",
      "Attach telemetry hooks",
      "Monitor lifecycle events",
      "Audit performance regression",
    ],
    href: "/guides/server-management",
  },
];
